ellipse.type = "convex",
ggtheme = theme_bw()
)
fviz_cluster(km.res, data = X,
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
install.packages("factoextra")
install.packages("installr")
library(installr)
updateR()
updateR()
source("libraries.R")
source("tweet-scraping.R")
source("data-transform.R")
# Load needed libraries and authenticate using Twitter tokens
load_libraries()
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
## designate packages to install/load
all_pkgs <- c("reticulate","png","RColorBrewer")
## find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
## designate packages to install/load
all_pkgs <- c("factoextra","tidyverse","twitteR", "tidytext")
## find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
# designate packages to install/load
all_pkgs <- c("factoextra","tidyverse","twitteR", "tidytext")
# find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
# now load all packages
sapply(all_pkgs, library, character.only=TRUE, logical.return=TRUE)
source('C:/Users/surfacepro/Desktop/twitter-k-means/setup.R', echo=TRUE)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# K means classifier
View(df)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# K means classifier
k_means <- kmeans(df,3,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
# K means classifier
k_means <- kmeans(df,2,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
# function to compute total within-cluster sum of square
wss <- function(k) {
kmeans(df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)
plot(k.values, wss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
fviz_nbclust(df, kmeans, method = "wss")
# function to compute total within-cluster sum of square
wss <- function(k) {
kmeans(df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)
plot(k.values, wss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
set.seed(123)
fviz_nbclust(df, kmeans, method = "silhouette")
# K means classifier
k_means <- kmeans(df,8,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
fviz_nbclust(df, kmeans, method = "silhouette")
fviz_gap_stat(gap_stat)
fviz_nbclust(df, kmeans, method = "silhouette")
# K means classifier
k_means <- kmeans(df,4,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "silhouette")
# K means classifier
k_means <- kmeans(df,4,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
# K means classifier
k_means <- kmeans(df,2,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
# K means classifier
clusters <- kmeans(df,2,nstart=25)
str(clusters)
str(clusters)
clusters$centers
# testing out different number of clusters
par(mfrow=c(2,2))
for (i in 1:4) {
clusters <- kmeans(df,i+1,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
}
fviz_cluster(clusters, data = df, labelsize=0)
for (i in 1:4) {
clusters <- kmeans(df,i+1,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
}
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,3,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,4,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,5,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
# testing out different number of clusters
par(mfrow=c(2,2))
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
# testing out different number of clusters
par(mfrow=c(2,2))
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,3,nstart=25)
a <- fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,3,nstart=25)
b <- fviz_cluster(clusters, data = df, labelsize=0)
c <- fviz_cluster(clusters, data = df, labelsize=0)
d <- fviz_cluster(clusters, data = df, labelsize=0)
a
b
clusters <- kmeans(df,2,nstart=25)
clusters <- kmeans(df,3,nstart=25)
clusters <- kmeans(df,2,nstart=25)
a <- fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,3,nstart=25)
b <- fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,4,nstart=25)
c <- fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,5,nstart=25)
d <- fviz_cluster(clusters, data = df, labelsize=0)
a
b
a
b
# testing out different number of clusters
plot_n_cluster <- function (x) {
clusters <- kmeans(df,x,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
}
n_clust <- 2:5
# testing out different number of clusters
plot_n_clusters <- function (x) {
clusters <- kmeans(df,x,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
}
n_clusters <- 2:5
map_dbl(n_clusters, plot_n_clusters)
a <- map_dbl(n_clusters, plot_n_clusters)
# testing out different number of clusters
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
fviz_cluster(clusters, data = df, labelsize=0)
fviz_cluster(clusters, data = df, labelsize=0)
# testing out different number of clusters
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
k2 <- kmeans(df,2,nstart=25)
k3 <- kmeans(df,3,nstart=25)
k4 <- kmeans(df,4,nstart=25)
k5 <- kmeans(df,5,nstart=25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
# determine optimal number of clusters
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "silhouette")
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "silhouette")
a <- fviz_nbclust(df, kmeans, method = "silhouette")
class(a)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 1000
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
# testing out different number of clusters
k2 <- kmeans(df,2,nstart=25)
df <- process_tweets(word,df_raw,n)
df_raw <- scrape_tweets(word, n)
View(df_raw)
View(df_raw)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
topic <- word
tweets <- df_raw
number_of_tweets <- n
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
# Create list of unwanted words
my_stop_words <- stop_words %>% select(-lexicon) %>%
bind_rows(exclude)
tweets$screenName
tweets$screenName
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
for (i in 1:number_of_tweets) {
user <- getUser(tweets$screenName[i])
followers <- append(followers, user$followersCount)
user_created <- append(user_created, as.Date(user$created))
location <- append(location, user$location)
total_tweets <- append(total_tweets, user$statusesCount)
}
tweets$followers <- followers
tweets$user_created <- user_created
tweets$location <- location
tweets$total_tweets <- total_tweets
# Separate each tweet by word
tweet_words <- tweets %>% select(id,text) %>% unnest_tokens(word,text)
# Filter out stop words
tweet_words_clean <- tweet_words %>% anti_join(my_stop_words)
common_words <- tweet_words_clean %>% count(word, sort=TRUE) %>% head(10)
# adding score column if tweet contains common words
score <- rep(0, number_of_tweets)
for (i in 1:nrow(common_words)) {
ids <- tweet_words_clean %>% filter(word==common_words[i,1]) %>% select(id) %>% unique()
for (j in 1:length(ids[,1])) {
n <- which(tweets$id == ids[j,1])
score[n] <- sum(score[n], 1)
}
}
# select only the numerical variable
tweets$score <- score
tweets_num <- select(tweets, total_tweets, followers, retweetCount, score)
# transform different categorical values into numerics
tweets_num$isRetweet <- as.numeric(tweets$isRetweet)
tweets_num$user_created <- as.numeric(tweets$user_created)
tweets_num$created <- as.numeric(tweets$created)
tweets_num$location <- as.numeric(as.factor(tweets$location))
scale(tweets_num)
View(scale(tweets_num))
df <- process_tweets(word,df_raw,n)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
df <- process_tweets(word,df_raw,n)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 400
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
n <- 50
# Choose topic word and create data frame used for algorithm
word <- "trump"
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 50
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# testing out different number of clusters
k2 <- kmeans(df,2,nstart=25)
k3 <- kmeans(df,3,nstart=25)
k4 <- kmeans(df,4,nstart=25)
k5 <- kmeans(df,5,nstart=25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
grid.arrange(p1, p2, p3, p4, nrow = 2)
# determine optimal number of clusters
set.seed(123)
fviz_nbclust(df, kmeans, method = "silhouette")
clusters <- kmeans(df,9,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
str(cluster)
str(clusters)
clusters$centers
# K means classifier
clusters <- kmeans(df,2,nstart=25)
fviz_cluster(clusters, data = df, labelsize=0)
str(clusters)
clusters$centers
str(clusters)
(p1, p2, p3, p4, nrow = 2)
grid.arrange(p1, p2, p3, p4, nrow = 2)
# (1) k1
k1
# (1) n_cluster = 2
k2
library(FNN)
install.packages("FNN")
library(FNN)
# (1) n_cluster = 2
k2$centers
set.seed(42)
x <- matrix(runif(150), 50, 3)
kmeans.x <- kmeans(x, 10)
kmeans.x
# (1) n_cluster = 2
get.knn(df, k2$centers,5)
# (1) n_cluster = 2
k2
y <- get.knn(df, k2$centers,5)
y <- get.knnx(df, k2$centers,5)
y
str(y)
y <- get.knnx(df, k2$centers,5)
y$nn.index[,1]
y$nn.index[1,]
y <-
ind2 <- get.knnx(df, k2$centers,5)$nn.index[1,]
# (1) n_cluster = 2
k2
ind2 <- get.knnx(df, k2$centers,5)$nn.index[1,]
ind2
source("tweet-scraping.R")
source("data-transform.R")
n <- 30
# Choose topic word and create data frame used for algorithm
word <- "trump"
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
df
df[1]
class(df[1])
class(df[2])
df[1]
df[[1]]
# scale and return dataframe
return(list(tweets, scale(tweets_num)))
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
class(df[1])
df[1]
df[[1]]
class(df[[1]])
df <- process_tweets(word,df_raw,n)
df[,1]
df[1]
df <- process_tweets(word,df_raw,n)
df <- process_tweets(word,df_raw,n)
source('C:/Users/surfacepro/Desktop/mine.R', echo=TRUE)
list(tweets, scale(tweets_num))
a <- list(tweets, scale(tweets_num))
a[1]
View(a[1])
View(as.dataframe(a[1]))
a <- list(tweets, scale(tweets_num))
a
a[1]
class(a[1])
as.data.frame(a[1])
a <- c(tweets, scale(tweets_num))
a[1]
class(a[1])
processed_data <- process_tweets(word,df_raw,n)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
processed_data <- process_tweets(word,df_raw,n)
tweets <- as.data.frame(process_data[1])
tweets <- as.data.frame(processed_data[1])
df <- as.data.frame(processed_data[2])
View(df)
View(tweets)
# obtaining indices of top 5 tweets nearest to centroid
ind2 <- get.knnx(df, k2$centers,5)$nn.index[1,]
# testing out different number of clusters
k2 <- kmeans(df,2,nstart=25)
k3 <- kmeans(df,3,nstart=25)
k4 <- kmeans(df,4,nstart=25)
k5 <- kmeans(df,5,nstart=25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
grid.arrange(p1, p2, p3, p4, nrow = 2)
# obtaining indices of top 5 tweets nearest to centroid
ind2 <- get.knnx(df, k2$centers,5)$nn.index[1,]
ind3 <- get.knnx(df, k3$centers,5)$nn.index[1,]
ind4 <- get.knnx(df, k4$centers,5)$nn.index[1,]
ind5 <- get.knnx(df, k5$centers,5)$nn.index[1,]
ind2
ind2
ind3 <- get.knnx(df, k3$centers,5)$nn.index[1,]
ind3
ind2
ind3
ind4 <- get.knnx(df, k4$centers,5)$nn.index[1,]
ind5 <- get.knnx(df, k5$centers,5)$nn.index[1,]
ind2
ind3
ind4
ind5
k2
k2$cluster
length(k2$cluster)
tweets$k2 <- k2$cluster
View(tweets)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
processed_data <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
processed_data <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
processed_data <- process_tweets(word,df_raw,n)
processed_data <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
df_raw <- scrape_tweets(word, n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 30
df_raw <- scrape_tweets(word, n)
