text(3,-2, paste("corr = ", round(cor(v$x,v$y),3)))
qqline(y)
phi<- function(x)    x^4*exp(x^2/4)
m  <-  10^6
sigma <- 1
mu <- 0
a <- 2
z <- rnorm(m)
z
is_est <- mean( h0*(z >a))
h0 <- phi(z)
h0is_est <- mean( h0*(z >a))
h0
f1=function(t){ t/(1+t*t)*exp(-(x-t)^2/2)}
f2=function(t){ 1/(1+t*t)*exp(-(x-t)^2/2)}
plot(f1,-3,3,col=1,ylim=c(-0.5,1),xlab="t",ylab="",ty="l")
plot(f2,-3,3,add=TRUE,col=2,ty="l")
legend("topright", c("f1=t.f2","f2"), lty=1,col=1 :2)
legend("topright", c("f1=t.f2","f2"), lty=1,col=1 :2)
f1=function(t){ t/(1+t*t)*exp(-(x-t)^2/2)}
f2=function(t){ 1/(1+t*t)*exp(-(x-t)^2/2)}
plot(f1,-3,3,col=1,ylim=c(-0.5,1),xlab="t",ylab="",ty="l")
plot(f2,-3,3,add=TRUE,col=2,ty="l")
legend("topright", c("f1=t.f2","f2"), lty=1,col=1 :2)
f1=function(t){ t/(1+t*t)*exp(-(x-t)^2/2)}
f2=function(t){ 1/(1+t*t)*exp(-(x-t)^2/2)}
plot(f1,-3,3,col=1,ylim=c(-0.5,1),xlab="t",ylab="",ty="l")
plot(f2,-3,3,add=TRUE,col=2,ty="l")
Niter=10^4
co=rcauchy(Niter)
I=mean(co*dnorm(co,mean=x))/mean(dnorm(co,mean=x))
I
x1=dnorm(co,mean=x)
estint2=cumsum(x1)/(1:Niter)
esterr2=sqrt(cumsum((x1-estint2)^2))/(1:Niter)
x1=co*x1
estint1=cumsum(x1))/(1:Niter)
esterr1=sqrt(cumsum((x1-estint1)^2))/(1:Niter)
par(mfrow=c(1,2))
plot(estint1,type="l",xlab="iteration",ylab="",col="gold")
lines(estint1-2*esterr1,lty=2,lwd=2)
lines(estint1+2*esterr1,lty=2,lwd=2)
plot(estint2,type="l",xlab="iteration",ylab="",col="gold")
lines(estint2-2*esterr2,lty=2,lwd=2)
x1=dnorm(co,mean=x)
estint2=cumsum(x1)/(1:Niter)
esterr2=sqrt(cumsum((x1-estint2)^2))/(1:Niter)
x1=co*x1
estint1=cumsum(x1))/(1:Niter)
estint1=cumsum(x1)/(1:Niter)
esterr1=sqrt(cumsum((x1-estint1)^2))/(1:Niter)
par(mfrow=c(1,2))
plot(estint1,type="l",xlab="iteration",ylab="",col="gold")
lines(estint1-2*esterr1,lty=2,lwd=2)
lines(estint1+2*esterr1,lty=2,lwd=2)
plot(estint2,type="l",xlab="iteration",ylab="",col="gold")
lines(estint2-2*esterr2,lty=2,lwd=2)
lines(estint2+2*esterr2,lty=2,lwd=2)
lines(estint2+2*esterr2,lty=2,lwd=2)
set.seed(1)
NSim <- 10^4
U <- runif(NSim)
X <- (-log(1-U)/lambda)^(1/3)
lambda <- 2
X <- (-log(1-U)/lambda)^(1/3)
mean.x=mean(x)
var.x=var(x)
x <- (-log(1-U)/lambda)^(1/3)
mean.x=mean(x)
var.x=var(x)
par(mfrow=c(1,1))
hist(x, freq=F, col= "grey", breaks =40,
main="Histogram: Generate random variable \
from F(x) using the inversion method")
set.seed(202009)
Nsim <- 10^4
set.seed(202009)
set.seed(1)
Nsim <- 10^4
r <- 2
f <- function(x) { dexp(x, 2)}
M <- optimize(f, interval =c(0,10), maximum =TRUE)$objective
M
y <- rexp(Nsim, r)
y <- rexp(Nsim, r)
u <- rexp(Nsim, r)
y <- rexp(Nsim, r)
u <- rexp(Nsim, r)
# since g(y)=1
x = y[u < dexp(y, r)/M ]
mean.x=mean(x)
mean.x
## [1] 0.6061904
# [1] 0.6061904
hist(x, freq=F, col= "grey", breaks =40, main="Compare simulation density of the true density")
s <- seq(0,1, length=100)
lines(s, dexp(x, r), lwd=2, col ="sienna")
s=seq(1,100,100)
s
s=seq(1,100,1)
s=seq(1,1.5,0.05)
s
s=seq(1,1.5,0.05)
s
s=seq(0,1.5,0.05)
s
s=seq(0,1.5,0.01)
s
matrix(c(0,0.5,0.5,0.5,0,0.5,0.5,0.5,0))
matrix(c(0,0.5,0.5,0.5,0,0.5,0.5,0.5,0),3,3)
m <-matrix(c(0,0.5,0.5,0.5,0,0.5,0.5,0.5,0),3,3)
m^1000
m^3
m^1000
m^1000
pexp(6, 1/4) - pexp(3,1/4)
x <- rexp(1000,1/4)
summary(x)
quantile(x,0.15)
qexp(0.15,1/4)
pexp(9.5,1/4)
1-pexp(9.5,1/4)
pnorm(0.75,0.64,0.09, lower.tail=FALSE)
pnorm(0.5,0.64,0.09)
qnorm(0.02,0.64,0.09,lower.tail=FALSE)
qnorm(1-0.02,0.64,0.09)
dbinom(6,26,0.5)
getwd()
read.csv("gender-classifier-DFE-791531.csv")
df <- read.csv("gender-classifier-DFE-791531.csv")
df <- read.csv("gender-classifier-DFE-791531.csv")
df <- read.csv("gender-classifier-DFE-791531.csv")
X <- data.frame(df$gender.confidence , df$tweet_count)
install.packages("factoextra")
library(factoextra)
km.res <- kmeans(df, 4, nstart=25)
km.res <- kmeans(X, 4, nstart=25)
X <- data.frame(df$gender.confidence , df$tweet_count)
head(df)
X <- na.rm(data.frame(df$gender.confidence , df$tweet_count))
km.res <- kmeans(X, 4, nstart=25)
head(x)
head(x, 10)
X <- matrxi(df$gender.confidence , df$tweet_count)
X <- matrix(df$gender.confidence , df$tweet_count)
head(x, 10)
names(df)
df$tweet_coord
X <- data.frame(col1 = df$gender.confidence, col2 = df$profile_yn.confidence)
head(x, 10)
head(X, 10)
X <- data.frame(col1 = df$gender.confidence, col2 = df$profile_yn.confidence, na.rm=TRUE)
head(X, 10)
X <- na.omit(data.frame(col1 = df$gender.confidence, col2 = df$profile_yn.confidence))
head(X, 10)
km.res <- kmeans(X, 2, nstart=25)
print(km.res)
km.res$cluster
library(ggpubr)
fviz_cluster(km.res, data = df[, -5],
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
fviz_cluster(km.res, data = X,
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
install.packages("factoextra")
install.packages("installr")
library(installr)
updateR()
updateR()
source("libraries.R")
source("tweet-scraping.R")
source("data-transform.R")
# Load needed libraries and authenticate using Twitter tokens
load_libraries()
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
## designate packages to install/load
all_pkgs <- c("reticulate","png","RColorBrewer")
## find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
## designate packages to install/load
all_pkgs <- c("factoextra","tidyverse","twitteR", "tidytext")
## find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
library(utils) #needed for the source to load installed.packages()
options(repos=c("https://cran.rstudio.com", getOption("repos") ) )
# designate packages to install/load
all_pkgs <- c("factoextra","tidyverse","twitteR", "tidytext")
# find packages that need to be installed
already_installed <- rownames(installed.packages())
to_install <- setdiff(all_pkgs, already_installed)
if (length(to_install) > 0) {
install.packages(to_install, dependencies=TRUE)
}
# now load all packages
sapply(all_pkgs, library, character.only=TRUE, logical.return=TRUE)
source('C:/Users/surfacepro/Desktop/twitter-k-means/setup.R', echo=TRUE)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
View(df)
"trump"
n <- 10
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
source("tweet-scraping.R")
source("data-transform.R")
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
# K means classifier
k_means <- kmeans(df,3,nstart=25)
fviz_cluster(k_means, data = df)
source('C:/Users/surfacepro/Desktop/twitter-k-means/setup.R', echo=TRUE)
#' Loads all corresponding packages
sapply(all_pkgs, library, character.only=TRUE, logical.return=TRUE)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
tweet <- scrape_tweets(word, n)
source("tweet-scraping.R")
source("data-transform.R")
tweet <- scrape_tweets(word, n)
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
tweet <- tweet_df
tweet <- tweet_df
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
topic <- word
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
# Create list of unwanted words
my_stop_words <- stop_words %>% select(-lexicon) %>%
bind_rows(exclude)
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
# Create list of unwanted words
my_stop_words <- stop_words %>% select(-lexicon) %>%
bind_rows(exclude)
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
# Create list of unwanted words
my_stop_words <- stop_words %>% select(-lexicon) %>%
bind_rows(exclude)
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
for (i in 1:number_of_tweets) {
user <- getUser(tweets$screenName[i])
followers <- append(followers, user$followersCount)
user_created <- append(user_created, as.Date(user$created))
location <- append(location, user$location)
total_tweets <- append(total_tweets, user$statusesCount)
}
tweets$followers <- followers
tweets$user_created <- user_created
tweets$location <- location
tweets$total_tweets <- total_tweets
tweets <- tweet_df
exclude <- data.frame(word = c(topic,
paste0(topic, c("'s" , "âs", "s")),
"https",
"t.co",
"rt",
"amp",
"itâs",
paste0(1:100)))
# Create list of unwanted words
my_stop_words <- stop_words %>% select(-lexicon) %>%
bind_rows(exclude)
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
for (i in 1:number_of_tweets) {
user <- getUser(tweets$screenName[i])
followers <- append(followers, user$followersCount)
user_created <- append(user_created, as.Date(user$created))
location <- append(location, user$location)
total_tweets <- append(total_tweets, user$statusesCount)
}
tweets$followers <- followers
number_of_tweets <- n
followers <- c()
user_created <- c()
location <- c()
total_tweets <- c()
for (i in 1:number_of_tweets) {
user <- getUser(tweets$screenName[i])
followers <- append(followers, user$followersCount)
user_created <- append(user_created, as.Date(user$created))
location <- append(location, user$location)
total_tweets <- append(total_tweets, user$statusesCount)
}
tweets$followers <- followers
tweets$user_created <- user_created
tweets$location <- location
tweets$total_tweets <- total_tweets
# Separate each tweet by word
tweet_words <- tweets %>% select(id,text) %>% unnest_tokens(word,text)
# Filter out stop words
tweet_words_clean <- tweet_words %>% anti_join(my_stop_words)
# Filter out stop words
tweet_words_clean <- tweet_words %>% anti_join(my_stop_words)
common_words <- tweet_words_clean %>% count(word, sort=TRUE) %>% head(10)
# adding score column if tweet contains common words
score <- rep(0, number_of_tweets)
for (i in 1:nrow(common_words)) {
ids <- tweet_words_clean %>% filter(word==common_words[i,1]) %>% select(id) %>% unique()
for (j in 1:length(ids[,1])) {
n <- which(tweets$id == ids[j,1])
score[n] <- sum(score[n], 1)
}
}
score
common_words
# select only the numerical variable
tweets$score <- score
tweets_num <- select(tweets, total_tweets, followers, retweetCount, score)
tweets_num$num_created <- as.numeric(tweets$created)
tweets_num$num_user_create <- as.numeric(tweets$user_created)
# rename rows to username and scale the data and rename the rows
row.names(tweets_num) <- make.names(tweets$score, unique=TRUE)
# select only the numerical variable
tweets$score <- score
tweets_num$num_created <- as.numeric(tweets$created)
tweets_num <- select(tweets, total_tweets, followers, retweetCount, score)
tweets_num$num_user_create <- as.numeric(tweets$user_created)
str(tweets_num)
str(tweets)
# K means classifier
k_means <- kmeans(df,2,nstart=25)
fviz_cluster(k_means, data = df)
# K means classifier
k_means <- kmeans(df,2,nstart=25)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
# K means classifier
k_means <- kmeans(df,2,nstart=25)
fviz_cluster(k_means, data = df)
# K means classifier
k_means <- kmeans(df,4,nstart=25)
fviz_cluster(k_means, data = df)
# K means classifier
k_means <- kmeans(df,3,nstart=25)
fviz_cluster(k_means, data = df)
str(tweets)
# transform different categorical values into numerics
table(tweets$isRetweet)
# transform different categorical values into numerics
table(tweets$retweeted)
# transform different categorical values into numerics
table(tweets$longitude)
tweets$created <- as.Date(tweets$created)
str(tweets)
# transform different categorical values into numerics
table(tweets$location)
# transform different categorical values into numerics
count(tweets$location)
# transform different categorical values into numerics
count(tweets, location, sort=T)
str(tweets)
tweets_num$isRetweet <- as.integer(tweets$isRetweet)
tweets_num$num_created <- as.numeric(tweets$created)
count(tweets, created, sort=T)
tweets <- tweet_df
number_of_tweets <- n
tweets$followers <- followers
tweets$user_created <- user_created
tweets$location <- location
tweets$total_tweets <- total_tweets
str(tweets)
count(tweets, created, sort=T)
tweets_num$num_user_create <- as.numeric(tweets$user_created)
tweets_num$user_create <- as.numeric(tweets$user_created)
count(tweets, user_create, sort=T)
count(tweets, user_created, sort=T)
tweets_num$user_created <- as.numeric(tweets$user_created)
count(tweets_num, user_created, sort=T)
tweets_num$created <- as.numeric(tweets$created)
count(tweets, created, sort=T)
count(tweets_num, created, sort=T)
str(tweets)
as.factor(tweets$location)
# transform different categorical values into numerics
tweets_num$isRetweet <- as.numeric(tweets$isRetweet)
count(tweets_num, isRetweet)
tweets_num$location <- as.factor(tweets$location)
count(tweets_num$location)
count(tweets_num, location)
tweets_num$location <- as.numeric(tweets$location)
tweets_num$location <- as.numeric(tweets$location)
tweets_num$location <- as.numeric(as.factor(tweets$location))
count(tweets_num, location)
tweets_num$location <- as.numeric(as.factor(tweets$location))
str(tweets)
names(tweets_num)
norw(tweets_num)
nrow(tweets_num)
str(tweets)
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
View(scale(tweets_num))
View(df)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
df <- process_tweets(word,tweet_df,n)
View(df)
# K means classifier
k_means <- kmeans(df,3,nstart=25)
fviz_cluster(k_means, data = df)
fviz_cluster(k_means, data = df, labelsize=0)
source('C:/Users/surfacepro/Desktop/twitter-k-means/setup.R', echo=TRUE)
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 100
tweet_df <- scrape_tweets(word, n)
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# K means classifier
k_means <- kmeans(df,3,nstart=25)
fviz_cluster(k_means, data = df, labelsize=0)
n <- 1000
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 500
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 500
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
# Choose topic word and create data frame used for algorithm
word <- "trump"
source("tweet-scraping.R")
source("data-transform.R")
# Choose topic word and create data frame used for algorithm
word <- "trump"
n <- 10
df_raw <- scrape_tweets(word, n)
df <- process_tweets(word,df_raw,n)
View(df_raw)
